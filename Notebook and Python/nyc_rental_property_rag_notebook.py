# -*- coding: utf-8 -*-
"""nyc-rental-property-rag-notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OBW8aQ_5NoP_3cdF796IH9bZMkk_WPEI
"""

!pip install transformers
!pip install faiss-cpu
!pip install datasets
!pip install streamlit

import pandas as pd
from transformers import DistilBertTokenizer, DistilBertModel
import torch
import faiss
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import streamlit as st
import re

data_path = '/kaggle/input/ny-rental-properties-pricing/NY Realstate Pricing.csv'
df = pd.read_csv(data_path)
df.head()
df_cleaned = df[['latitude', 'longitude', 'neighbourhood', 'room_type', 'price', 'days_occupied_in_2019', 'minimum_nights', 'number_of_reviews']]
df_cleaned.dropna(inplace=True)
df_cleaned.head()

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

def get_embeddings(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)
    with torch.no_grad():
        embeddings = model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings


sample_texts = df_cleaned['neighbourhood'].head(10).tolist()
embeddings = get_embeddings(sample_texts)
embeddings_np = embeddings.numpy()

index = faiss.IndexFlatL2(embeddings_np.shape[1])
index.add(embeddings_np)

gpt_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt_model = GPT2LMHeadModel.from_pretrained("gpt2")
def generate_response(query, context):
    input_text = f"Context: {context}\nQuestion: {query}\nAnswer:"
    inputs = gpt_tokenizer.encode(input_text, return_tensors="pt")
    outputs = gpt_model.generate(inputs, max_length=512, num_return_sequences=1, no_repeat_ngram_size=2)
    return gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)


def limit_response_length(response, max_length=1500):
    truncated_response = response[:max_length]
    last_punctuation_pos = max(truncated_response.rfind(punc) for punc in ['.', '!', '?'])
    if last_punctuation_pos != -1:
        truncated_response = truncated_response[:last_punctuation_pos + 1]
    else:
        truncated_response = truncated_response.rstrip().rsplit(' ', 1)[0]
    return truncated_response.strip()


def get_relevant_context(query, top_k=3):
    query_embedding = get_embeddings([query]).numpy()
    distances, indices = index.search(query_embedding, top_k)
    contexts = df_cleaned['neighbourhood'].iloc[indices[0]].tolist()
    return " ".join(contexts)

def answer_query(query):
    context = get_relevant_context(query)
    answer = generate_response(query, context)
    answer = limit_response_length(answer)
    return answer

query = "What is the average rent in the Brooklyn neighborhood?"
response = answer_query(query)
print(response)

st.title('NY Rental Properties RAG Model')

if 'history' not in st.session_state:
    st.session_state.history = []

def handle_query(query):
    answer = answer_query(query)
    st.session_state.history.append({"query": query, "answer": answer})

for entry in st.session_state.history:
    st.write(f"**Question**: {entry['query']}")
    st.write(f"**Answer**: {entry['answer']}")
    st.write("---")

query = st.text_input('Ask a question about NY rental properties:')

if query:
    handle_query(query)
    st.experimental_rerun()

queries_and_responses = []
def test_query(query):
    response = answer_query(query)
    queries_and_responses.append({"query": query, "answer": response})

queries = [
    "What is the average rent in Brooklyn?",
    "What are the most popular room types?",
    "What is the price range for a 2-Bedroom apartment?",
    "How many reviews are there on properties in Manhattan?"
]

for query in queries:
    test_query(query)

for entry in queries_and_responses:
    print(f"**Question**: {entry['query']}")
    print(f"**Answer**: {entry['answer']}")
    print("-" * 50)